stages:
  - review
  - build
  - extract
  - model
  - update
  - review_stop

.job_template: &job_definition
  image: registry.gitlab.com/bizops/bizops-elt/extract:latest
  tags:
    - analytics

.marketo_extract: &marketo_extract
  stage: extract
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - bash processes/mkto_processor.sh
    - stop_cloudsqlproxy

# Stage: extract

## Production Jobs

sfdc_extract:
  variables:
    FIXED_SFDC_PASSWORD: $SFDC_PASSWORD
  stage: extract
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - envsubst < "elt/config/kettle/kettle.properties" > "$KETTLE_HOME/.kettle/kettle.properties"
    - cp "elt/config/kettle/repositories.xml" "$KETTLE_HOME/.kettle/repositories.xml"
    - kitchen.sh -file=elt/sfdc/sfdc_extract.kjb -level=Minimal
    - stop_cloudsqlproxy

zuora:
  variables:
    FIXED_ZUORA_PASSWORD: $ZUORA_PASSWORD
  stage: extract
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - envsubst < "elt/config/environment.conf.template" > "elt/config/environment.conf"
    - python3 elt/zuora/zuora_export.py
    - stop_cloudsqlproxy

pings:
  stage: extract
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - bash processes/hosts_to_sfdc/python_timecheck.sh
    - stop_cloudsqlproxy

marketo_extract:
  <<: *marketo_extract
  only:
    - master

# Manual Jobs

marketo_extract_manual:
  <<: *marketo_extract
  only:
    - branches
  except:
    - master
  when: manual

# Stage: model

dbt:
  stage: model
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - cd elt/dbt
    - dbt -d run --profiles-dir profile --target prod --models sfdc
    - dbt -d run --profiles-dir profile --target version --models pings
    - dbt -d test --profiles-dir profile
    - stop_cloudsqlproxy

sfdc_update:
  stage: update
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - python3 processes/sfdc_processor.py
    - stop_cloudsqlproxy

sfdc_snapshot:
  stage: update
  <<: *job_definition
  script:
    - set_sql_instance_name
    - setup_cloudsqlproxy
    - python3 elt/util/snapshot.py
    - stop_cloudsqlproxy

# Stage: review 

review:
  stage: review
  image: google/cloud-sdk:latest
  variables:
    GIT_STRATEGY: none
  tags:
    - analytics
  script:
    - set_sql_instance_name
    - manage_review_cloudsql
  environment:
    name: review/$CI_COMMIT_REF_NAME
    on_stop: review_stop
  only:
    - branches
  except:
    - master

# Stage: review_stop

review_stop:
  stage: review_stop
  image: google/cloud-sdk:latest
  variables:
    GIT_STRATEGY: none
  script: 
    - delete_review_cloudsql
  when: manual
  allow_failure: true
  only:
    - branches
  except:
    - master
  environment:
    name: review/$CI_COMMIT_REF_NAME
    action: stop

# ---------------------------------------------------------------------------

.bizops: &bizops |

  function set_sql_instance_name() {
    if [ -n "$GCP_SERVICE_CREDS" ]; then
      if [ "$CI_COMMIT_REF_NAME" = "master" ]; then
        export GCP_INSTANCE_NAME=$GCP_PRODUCTION_INSTANCE_NAME
      else
        export GCP_INSTANCE_NAME=$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHA
      fi
      echo "Instance name is: $GCP_INSTANCE_NAME"
    fi
  }

  function manage_review_cloudsql() {
    # Check to see if branch name equals the production EDW instance name
    if [ "$CI_COMMIT_REF_NAME" =  "$GCP_PRODUCTION_INSTANCE_NAME" ]; then
      echo "The branch name cannot match the production EDW instance name."
      return 1;
    fi


    if [ -n "$GCP_SERVICE_CREDS" ]; then
      echo $GCP_SERVICE_CREDS > gcp_credentials.json
      gcloud auth activate-service-account --key-file=gcp_credentials.json

      # Delete old instances for this branch
      echo "Cleaning up old Cloud SQL instances for this branch."
      gcloud sql instances list --project "$GCP_PROJECT" --filter "$CI_COMMIT_REF_SLUG" | tail -n +2 | cut -d' ' -f1 | xargs -I % gcloud sql instances delete --project "$GCP_PROJECT" %

      # Clone new instance and wait for it
      echo "Cloning database..."
      gcloud sql instances clone --project "$GCP_PROJECT" "$GCP_PRODUCTION_INSTANCE_NAME" "$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHA" |& grep -E -o '[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}' | head -n1 | xargs -I % gcloud beta sql operations wait --project "$GCP_PROJECT" % || true

    fi
  }

  function delete_review_cloudsql() {
    # Check to see if branch name equals the production EDW instance name
    if [ "$CI_COMMIT_REF_NAME" =  "$GCP_PRODUCTION_INSTANCE_NAME" ]; then
      echo "The branch name cannot match the production EDW instance name."
      return 1;
    fi

    if [ -n "$GCP_SERVICE_CREDS" ]; then
      echo $GCP_SERVICE_CREDS > gcp_credentials.json
      gcloud auth activate-service-account --key-file=gcp_credentials.json
      gcloud sql instances delete --project "$GCP_PROJECT" "$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHA"
    fi
  }

  function setup_cloudsqlproxy() {
    if [ -n "$GCP_SERVICE_CREDS" ]; then
      echo $GCP_SERVICE_CREDS > /bizops/gcp_credentials.json
      cloud_sql_proxy -instances="$GCP_PROJECT:$GCP_REGION:$GCP_INSTANCE_NAME"=tcp:5432 -credential_file=/bizops/gcp_credentials.json -verbose=False &
    fi
  }

  function stop_cloudsqlproxy() {
    if [ -n "$GCP_SERVICE_CREDS" ]; then
      kill -9 `pgrep cloud_sql_proxy`
    fi
  }

before_script:
  - *bizops
